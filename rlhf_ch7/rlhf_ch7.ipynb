{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBTsj25y1QMe",
        "outputId": "769d2037-b64b-4c3a-f63a-d248910e8183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnHCP73d1abG",
        "outputId": "6cfb8959-4751-43ee-c527-f8d5af7ee47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/upskilling/transformers_ch1\n"
          ]
        }
      ],
      "source": [
        "%cd drive/My Drive/Colab Notebooks/upskilling/transformers_ch1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmC7edL_1cdt"
      },
      "source": [
        "# Chapter 7 - Reinforcement Learning from Human Feedback\n",
        "\n",
        "* Practical exercise: fine-tune your Shakespeare transformer from Chapter 1 using RLHF to get it to output samples with positive sentiment. (If you didn't do that exercise, you might be able to find some code to help you from the solutions page).\n",
        "\n",
        "    * Collect some unconditional samples from your model, and label them as positive, negative or neutral in sentiment yourself. You will probably need at least few hundred labels, so keep the samples fairly short so that this isn't too laborious. (Consider pooling your labels if working with others, or mixing in some snippets from the original corpus whose sentiment is less ambiguous.)\n",
        "    * Fine-tune your model to obtain a reward model that predicts the sentiment of a sample. You can treat this as a sequence-modeling problem by having a model predict special tokens such as \"happy\" and \"sad\" based on the sentiment, treating neutral labels as soft 50% labels. Remember to mask all but the last token of the sample.\n",
        "    * Fine-tune your original model using PPO, with rewards given by the log probability of positive sentiment predicted by the reward model.\n",
        "        * To make things simpler, don't bother with a value function or GAE. Just use the reward itself as the advantage estimate at every token.\n",
        "        * Recenter and normalize the rewards. Just use fixed constants taken from a few thousand samples instead of implementing running estimates.\n",
        "        * As in chapter 6, track the fraction of ratios clipped. If it is below 1%, then increase the iteration batch size, i.e., the number of samples in each alternation between rollouts and optimization. The iteration batch size might need to be a few thousand completions or more.\n",
        "        * Measure KL(current model || original model). If the fraction of ratios clipped is high enough, you shouldn't need to penalize this directly to prevent it growing too fast. Square root KL should grow roughly linearly over the course of training. Stop training once you reach 10 nats per completion, but keep hold of plenty of intermediate checkpoints at lower KLs.\n",
        "    * Look at some samples from your different checkpoints, and try to get a sense of which one is the best, and where overoptimization started to occur.\n",
        "    * Evaluate your preferred model by blindly rating 20 samples from your original model and 20 from your final model, to see whether it really is better. Maybe even ask a friend to do the ratings in case you think you can recognize the model. You can also try comparing it to a model that was trained to optimize negative sentiment (note: don't flip the sign of the reward function when training AGI).\n",
        "    * Extension/alternative: do the same thing but with conditional rather than unconditional samples (using prompts from the original Shakespeare dataset), and training a comparison reward model instead of an absolute reward model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GgQx2SJY1c9w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# code\n",
        "from scripts.models import *\n",
        "from scripts.config import Configs\n",
        "\n",
        "# data\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import urllib3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aQ5B5J091kLN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AeE4l78m1hJb"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
        "http = urllib3.PoolManager()\n",
        "data = http.request('GET', url)\n",
        "soup = BeautifulSoup(data.data)\n",
        "soup_str = str(soup)\n",
        "\n",
        "# cleaning text\n",
        "def clean_text(string):\n",
        "\n",
        "  # remove \\r and \\n\n",
        "  string = re.sub(r'\\r', ' ', string)\n",
        "  string = re.sub(r'\\n', ' ', string)\n",
        "\n",
        "  # remove characters between <> ad []\n",
        "  string = re.sub(r'<[^>]+>', ' ', string)\n",
        "  string = re.sub(r'\\[[^\\]]+\\]', ' ', string)\n",
        "\n",
        "  # remove spaces greater than one\n",
        "  string = re.sub(r'\\s+', ' ', string)\n",
        "\n",
        "  return string\n",
        "\n",
        "# tokenizer function\n",
        "def tokenizer(clean_text):\n",
        "\n",
        "  toks = re.split(r'\\b', clean_text)\n",
        "  toks = [i.strip() for i in toks if i != ' ']\n",
        "\n",
        "  return np.array(toks)\n",
        "\n",
        "# creating tokens and vocab size\n",
        "clean_soup = clean_text(soup_str)\n",
        "toks = tokenizer(clean_soup)\n",
        "vocab = list(set(toks))\n",
        "vocab.append('[UNKNOWN]')\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "# create dictionaries\n",
        "id2tok = dict(enumerate(vocab))\n",
        "tok2id = {v:k for k, v in id2tok.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwgHWVdf_R-g",
        "outputId": "faae5009-5398-417b-b99b-2ee115903b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31316\n"
          ]
        }
      ],
      "source": [
        "print(VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdwYlilG1nZz",
        "outputId": "342c7b15-dee4-4e22-eef2-339581d2c2d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "cfg = Configs(BATCH_SIZE=64, SEQ_LEN=128, N_LAYERS=6)\n",
        "BardofAvon = Transformer(vocab_size=VOCAB_SIZE, mask=True, config=cfg).to(device)\n",
        "BardofAvon.load_state_dict(torch.load('bardofavon.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeNenwCM1_T3"
      },
      "source": [
        "## Samples\n",
        "\n",
        "I will use the samples collected by ckkissane in (https://github.com/ckkissane/rlhf-shakespeare/blob/main/rlhf_shakespeare/data/handcrafted_data.jsonl)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_5EHfjB5GiS",
        "outputId": "012eaecb-b63d-4f51-c44e-87b410d9b4bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "! pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z7mmQiB52L-K"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "objs = []\n",
        "with jsonlines.open('handcrafted_data.jsonl') as reader:\n",
        "            for obj in reader:\n",
        "                objs.append(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fofVyKZf5OJ7"
      },
      "outputs": [],
      "source": [
        "dataset = {'text':[], 'label':[]}\n",
        "for i in objs:\n",
        "  dataset['text'].append(i['sample'])\n",
        "  dataset['label'].append(i['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ddb43fv85oUW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg0qHKgj20x8"
      },
      "source": [
        "## Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xoywxYTxAXFN"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XeJcJmlS--h5"
      },
      "outputs": [],
      "source": [
        "class RewardTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, pretrained, classes=2):\n",
        "    super(RewardTransformer, self).__init__()\n",
        "\n",
        "    self.embed = copy.deepcopy(pretrained.embedding)\n",
        "    self.posenc = copy.deepcopy(pretrained.pos_enc)\n",
        "    self.drop = copy.deepcopy(pretrained.dropout)\n",
        "    self.blocks = copy.deepcopy(pretrained.blocks)\n",
        "    self.ff1 = nn.Linear(512, 512)\n",
        "    self.ff2 = nn.Linear(512, classes)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    emb = self.embed(X)\n",
        "    out = self.drop(self.posenc(emb))\n",
        "\n",
        "    for i, l in enumerate(self.blocks):\n",
        "\n",
        "      out = l(out)\n",
        "\n",
        "    out = self.ff1(out)\n",
        "\n",
        "    return self.ff2(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using bert and then fine tunning it"
      ],
      "metadata": {
        "id": "0yfdN02BAvMf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9J1B0any04Rj"
      },
      "outputs": [],
      "source": [
        "class RewardModelData(Dataset):\n",
        "\n",
        "  def __init__(self, samples, tok2id, seq_len = 128, num_classes=2):\n",
        "\n",
        "    self.tok2id = tok2id\n",
        "    self.id2tok = {v:k for k, v in self.tok2id.items()}\n",
        "\n",
        "    self.sequences = []\n",
        "\n",
        "    for i in range(samples.shape[0]):\n",
        "\n",
        "      # clean and tokenize\n",
        "      text = clean_text(samples['text'][i])\n",
        "      text_toks = tokenizer(text)\n",
        "\n",
        "      # try and get token id else if token not in dict\n",
        "      toks_ids = []\n",
        "      not_found_counter = 0\n",
        "      exceptions = []\n",
        "      for t in text_toks:\n",
        "        try:\n",
        "          id = self.tok2id[t]\n",
        "          toks_ids.append(id)\n",
        "        except:\n",
        "          not_found_counter += 1\n",
        "          exceptions.append(t)\n",
        "\n",
        "      # print how many samples were not found\n",
        "      if not_found_counter != 0:\n",
        "        print('Sample', i, 'found', not_found_counter, 'exceptions = ', exceptions)\n",
        "\n",
        "      # pad\n",
        "      if len(toks_ids) < seq_len:\n",
        "        toks_ids.extend([self.tok2id['[UNKNOWN]'] for _ in range(len(toks_ids), seq_len)])\n",
        "      toks_ids = torch.Tensor(toks_ids).type(torch.LongTensor)\n",
        "\n",
        "      # limit len\n",
        "      toks_ids = toks_ids[:seq_len]\n",
        "\n",
        "      # make label - masking all but the last token\n",
        "      label = -1000 * torch.ones(seq_len)\n",
        "      label_val = 1 if samples['label'][i] == 'happy' else 0\n",
        "      label[-1] = label_val\n",
        "      label = label.type(torch.LongTensor)\n",
        "\n",
        "      self.sequences.append((toks_ids, label))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    text, label = self.sequences[idx]\n",
        "\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "72wPQOtZCYM-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "train_size = int(0.7 * df.shape[0])\n",
        "df_train, df_test = df[:train_size], df[train_size:]\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBO3j7Mp-7fF",
        "outputId": "4f9f9bbc-ae99-4651-92a4-56b238fed752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 3 found 1 exceptions =  ['Howlings']\n",
            "Sample 10 found 1 exceptions =  ['engrav']\n",
            "Sample 23 found 2 exceptions =  ['Exeunt', '']\n",
            "Sample 33 found 1 exceptions =  ['']\n",
            "Sample 43 found 1 exceptions =  ['']\n",
            "Sample 78 found 1 exceptions =  ['PHEBE']\n",
            "Sample 80 found 1 exceptions =  ['Launce']\n",
            "Sample 81 found 1 exceptions =  ['Goddild']\n",
            "Sample 82 found 1 exceptions =  ['LAUNCE']\n",
            "Sample 103 found 1 exceptions =  ['sluggardiz']\n",
            "Sample 114 found 1 exceptions =  ['']\n",
            "Sample 120 found 1 exceptions =  ['']\n",
            "Sample 122 found 1 exceptions =  ['Phebe']\n",
            "Sample 123 found 1 exceptions =  ['ador']\n",
            "Sample 137 found 1 exceptions =  ['']\n",
            "Sample 143 found 1 exceptions =  ['']\n"
          ]
        }
      ],
      "source": [
        "train_dset = RewardModelData(df_train, tok2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjPLDZ4y_CIL",
        "outputId": "393f0a71-cea9-4f0b-9f9a-c346714183c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 7 found 1 exceptions =  ['']\n",
            "Sample 15 found 1 exceptions =  ['']\n",
            "Sample 22 found 1 exceptions =  ['']\n",
            "Sample 36 found 1 exceptions =  [\". '\"]\n",
            "Sample 43 found 1 exceptions =  ['']\n",
            "Sample 44 found 1 exceptions =  [\"! '\"]\n",
            "Sample 46 found 1 exceptions =  ['']\n",
            "Sample 58 found 1 exceptions =  ['']\n",
            "Sample 60 found 1 exceptions =  ['Audre']\n"
          ]
        }
      ],
      "source": [
        "test_dset = RewardModelData(df_test, tok2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xruu5rgdDdeJ"
      },
      "outputs": [],
      "source": [
        "train_dlr = DataLoader(train_dset, shuffle=True, batch_size=4)\n",
        "test_dlr = DataLoader(test_dset, shuffle=True, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U87vbaWXEN04"
      },
      "outputs": [],
      "source": [
        "reward_model = RewardTransformer(BardofAvon).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxxqMi3Cnl-d"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SV2vTHShGDfy"
      },
      "outputs": [],
      "source": [
        "text, labels = next(iter(train_dlr))\n",
        "text = text.to(device)\n",
        "labels = labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmYMu-rAFC6B",
        "outputId": "6d174d0a-bc4d-4a43-e376-3eb1609987ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 128, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "out = reward_model(text)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(out[:, -1, :][:, -1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnYye-Oksxzp",
        "outputId": "7c4e5ade-239c-4cb8-f26f-527d81514d63"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-9b8bdd1a89d8>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  F.softmax(out[:, -1, :][:, -1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1659, 0.2291, 0.2608, 0.3441], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtd9OFkedVmO",
        "outputId": "1dc1e0bf-bd37-42c6-9bab-f5e5b1b64577"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7544, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "F.cross_entropy(out.view(-1, out.shape[-1]), labels.view(-1), ignore_index=-1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqoHtqsdnof3"
      },
      "source": [
        "## Training Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "18J18eG7r2x9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1tcWAGQKdjuC"
      },
      "outputs": [],
      "source": [
        "def train(model, dtlr, optm, epochs):\n",
        "\n",
        "  running_train_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  for e in range(epochs):\n",
        "    pbar = tqdm(dtlr, total=len(dtlr))\n",
        "    running_train_loss = 0\n",
        "    for t, l in pbar:\n",
        "\n",
        "      t = t.to(device)\n",
        "      l = l.to(device)\n",
        "\n",
        "      # run training\n",
        "      logits = model(t)\n",
        "      optm.zero_grad()\n",
        "      l = F.cross_entropy(logits.view(-1, logits.shape[-1]), l.view(-1), ignore_index=-1000)\n",
        "      running_train_loss += l.item()\n",
        "      l.backward()\n",
        "      optm.step()\n",
        "\n",
        "    print('Epoch: ', e, 'Training Loss: ', running_train_loss/len(dtlr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VpjpoLJvvwXX"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "optm = torch.optim.Adam(reward_model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdyaksYCwDQb",
        "outputId": "948fab7f-8019-4ba2-a270-b63ff0f6803d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:11<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Training Loss:  1.0611085888553173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:08<00:00,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 Training Loss:  0.6069934961009533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(reward_model, train_dlr, optm, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yt9KdKG_wImp"
      },
      "outputs": [],
      "source": [
        "reward_model.eval()\n",
        "ypreds = []\n",
        "labels = []\n",
        "for t, l in train_dlr:\n",
        "\n",
        "  t = t.to(device)\n",
        "  mask = l.view(-1) != -1000\n",
        "  logits = reward_model(t)\n",
        "  preds = logits.argmax(dim=-1)\n",
        "  preds = preds.view(-1)[mask].cpu().numpy()\n",
        "  true_label = l.view(-1)[mask].cpu().numpy()\n",
        "  ypreds.append(preds)\n",
        "  labels.append(true_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGRyBE7bAzsl",
        "outputId": "b7d224c3-c45e-4066-9b5e-1bf86161f750"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "sum(ypreds[0] == labels[0])/len(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r7XoLY0AZ_n",
        "outputId": "7731f661-2791-44c2-de51-b709a1e8c940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation accuracy: 0.6666666865348816\n"
          ]
        }
      ],
      "source": [
        "reward_model.eval()\n",
        "correct, total = 0, 0\n",
        "for test_x, test_y in test_dlr:\n",
        "    test_logits = reward_model(test_x.to(device))\n",
        "    pred = test_logits.argmax(dim=-1)\n",
        "    correct += (pred == test_y.to(device)).sum()\n",
        "    total += test_x.shape[0]\n",
        "print(f\"validation accuracy: {correct / total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgPuhiOX2_UJ"
      },
      "source": [
        "## PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xTU9pU5773ly"
      },
      "outputs": [],
      "source": [
        "from torch.distributions.categorical import Categorical as C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "W2v59Dr_0Q6b"
      },
      "outputs": [],
      "source": [
        "class PPOAgent(nn.Module):\n",
        "\n",
        "  def __init__(self, pretrained):\n",
        "    super(PPOAgent, self).__init__()\n",
        "\n",
        "    self.model = copy.deepcopy(pretrained)\n",
        "\n",
        "  def get_policy(self, X):\n",
        "\n",
        "    logits = self.model(X)\n",
        "    policy = C(logits=logits)\n",
        "\n",
        "    return policy\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    logits = self.model(X)\n",
        "    logits = logits[:, -1, :]\n",
        "    policy = C(logits=logits)\n",
        "    action = policy.sample()\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "cq9_uW-IOBf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt, label = next(iter(train_dlr))\n",
        "test_agent = PPOAgent(BardofAvon).to(device)\n",
        "a = test_agent.get_policy(txt.to(device))"
      ],
      "metadata": {
        "id": "RKF8AGBYDMrB"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = test_agent.predict(txt.to(device))"
      ],
      "metadata": {
        "id": "V0aHXt-_DmF5"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui1u0eQySnTk",
        "outputId": "5fefb8d2-9344-4a21-c96e-88c0567e3a9f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lSmvmCJKfYU",
        "outputId": "6b7f65d6-ea5c-4d1b-949e-a28a2e9cae2e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15034, 18240,  9521, 25507], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our loss function"
      ],
      "metadata": {
        "id": "8f67BnK1N7df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "xhylPh2i8y0i"
      },
      "outputs": [],
      "source": [
        "class PPOLoss(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PPOLoss, self).__init__()\n",
        "\n",
        "  def clippedSurrogateObjective(self, A, log_policy, log_old_policy, epsilon):\n",
        "    r = torch.exp(log_policy - log_old_policy)\n",
        "    first = -A*r\n",
        "    second = -A*torch.clamp(r, 1-epsilon, 1+epsilon)\n",
        "    L_clip = torch.max(first, second)\n",
        "    return -L_clip.mean()\n",
        "\n",
        "  def get_info(self,\n",
        "               log_policy,\n",
        "               log_old_policy,\n",
        "               epsilon=0.1):\n",
        "    r = torch.exp(log_policy - log_old_policy)\n",
        "    clipped = r.gt(1.0 + epsilon) | r.lt(1 - epsilon)\n",
        "    clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
        "    approx_kl = (0.5 * (log_policy - log_old_policy) ** 2).mean().item()\n",
        "\n",
        "    pi_info = dict(kl=approx_kl, cf=clipfrac)\n",
        "    return pi_info\n",
        "\n",
        "  def forward(self, adv,\n",
        "               log_policy,\n",
        "               log_old_policy,\n",
        "               epsilon=0.1):\n",
        "\n",
        "    lclip = self.clippedSurrogateObjective(adv, log_policy, log_old_policy, epsilon)\n",
        "    pi_info = self.get_info(log_policy, log_old_policy, epsilon)\n",
        "\n",
        "    print('Clip Loss:', lclip)\n",
        "\n",
        "    return lclip, pi_info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPOStorage class to store important tensors generated during rollout."
      ],
      "metadata": {
        "id": "gTkvZYDANyMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOStorage:\n",
        "\n",
        "  def __init__(self, seq_len, batch_size):\n",
        "    self.seq_len = seq_len\n",
        "    self.batch_size = batch_size\n",
        "    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    self.start()\n",
        "\n",
        "  def start(self):\n",
        "    self.states_b = torch.zeros((self.batch_size, self.seq_len)).type(torch.LongTensor).to(self.device)\n",
        "    self.actions_b = torch.zeros((self.batch_size, self.seq_len)).type(torch.LongTensor).to(self.device)\n",
        "    self.logprobs_b = torch.zeros((self.batch_size, self.seq_len)).type(torch.LongTensor).to(self.device)\n",
        "    self.rewards_b = torch.zeros((self.batch_size,)).type(torch.LongTensor).to(self.device)\n",
        "    self.adv_b = torch.zeros((self.batch_size, self.seq_len)).type(torch.LongTensor).to(self.device)\n",
        "\n",
        "  def update(self, states, actions, logprobs, rewards, adv, batch):\n",
        "\n",
        "    self.states_b[batch] = torch.Tensor(states).type(torch.LongTensor).to(self.device)\n",
        "    self.actions_b[batch] = torch.Tensor(actions).type(torch.LongTensor).to(self.device)\n",
        "    self.logprobs_b[batch] = torch.Tensor(logprobs).type(torch.LongTensor).to(self.device)\n",
        "    self.rewards_b[batch] = torch.Tensor(rewards).type(torch.LongTensor).to(self.device)\n",
        "    self.adv_b[batch] = torch.Tensor(adv).type(torch.LongTensor).to(self.device)\n",
        "\n",
        "    if batch == self.batch_size - 1:\n",
        "      print('Advantage Normalized')\n",
        "      self.normalize_adv()\n",
        "\n",
        "  def normalize_adv(self):\n",
        "    self.adv_b = self.adv_b.float()\n",
        "    self.adv_b = (self.adv_b - self.adv_b.mean()) / self.adv_b.std()"
      ],
      "metadata": {
        "id": "SrCBik_TmByU"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "\n",
        "  def __init__(self,\n",
        "               seq_len,\n",
        "               vocab,\n",
        "               id2tok,\n",
        "               tok2id,\n",
        "               storage,\n",
        "               ppoagent,\n",
        "               initial_agent,\n",
        "               reward_model,\n",
        "               optimizer,\n",
        "               loss,\n",
        "               device,\n",
        "               num_its=50,\n",
        "               epochs=4,\n",
        "               batch_size=4):\n",
        "\n",
        "    self.seq_len = seq_len\n",
        "    self.vocab = vocab\n",
        "    self.id2tok = id2tok\n",
        "    self.tok2id = tok2id\n",
        "    self.storage = storage\n",
        "    self.ppoagent = ppoagent.to(device)\n",
        "    self.initial_agent = initial_agent.to(device)\n",
        "    self.reward_model = reward_model.to(device)\n",
        "    self.batch_size = batch_size\n",
        "    self.optimizer = optimizer\n",
        "    self.loss = loss.to(device)\n",
        "    self.num_its = num_its\n",
        "    self.epochs = epochs\n",
        "    self.device = device\n",
        "\n",
        "  def rollout(self, model, batch_num, store=True):\n",
        "\n",
        "    # phrase\n",
        "    initial_sample_word = np.random.choice(self.vocab, 1)\n",
        "    initial_sample = ['[UNKNOWN]' for _ in range(self.seq_len - 1)]\n",
        "    initial_sample.append(initial_sample_word[0])\n",
        "    initial_sample = [self.tok2id[w] for w in initial_sample]\n",
        "    actions = []\n",
        "\n",
        "    initial_sample = torch.tensor(initial_sample).to(self.device)\n",
        "    initial_sample = initial_sample[None, :]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for _ in range(self.seq_len):\n",
        "        next_word = model.predict(initial_sample)\n",
        "        actions.append(next_word)\n",
        "        initial_sample = torch.cat((initial_sample, next_word.unsqueeze(0)), dim=-1)\n",
        "        initial_sample = initial_sample[:, 1:]\n",
        "\n",
        "\n",
        "    if store:\n",
        "      actions = torch.tensor(actions).to(self.device)\n",
        "      rewards = self.reward_model(initial_sample)\n",
        "      rewards = F.softmax(rewards[:, -1, :], dim=-1)[:, 1]\n",
        "      rewards = torch.log(rewards)\n",
        "      advs = rewards * torch.ones(self.seq_len).to(self.device)\n",
        "      log_prob = model.get_policy(initial_sample).log_prob(actions).squeeze()\n",
        "      self.storage.update(initial_sample, actions, log_prob, rewards, advs, batch_num)\n",
        "\n",
        "    phrase = ' '.join([self.id2tok.get(w.item(), '[UNK]') for w in initial_sample.flatten()])\n",
        "\n",
        "    return phrase\n",
        "\n",
        "  def full_rollout(self):\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "\n",
        "      phrase = self.rollout(self.ppoagent, i)\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    self.ppoagent.train()\n",
        "    for _ in range(self.num_its):\n",
        "      initial_phrase = self.rollout(self.initial_agent, 0, store=False)\n",
        "      print('INITIAL PHRASE:', initial_phrase)\n",
        "      self.full_rollout()\n",
        "\n",
        "      for e in range(self.epochs):\n",
        "        print('TRAIN EPOCH', e)\n",
        "        self.optimizer.zero_grad()\n",
        "        n_log_prob = self.ppoagent.get_policy(self.storage.states_b).log_prob(self.storage.actions_b)\n",
        "        loss, pi_info = self.loss(self.storage.adv_b, n_log_prob, self.storage.logprobs_b)\n",
        "        print('KL:', pi_info['kl'], '\\n', 'Clip Frac:', pi_info['cf'])\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      after_roll = self.rollout(self.ppoagent, 0, store=False)\n",
        "      print('AFTER TRAIN PHRASE:', after_roll)\n"
      ],
      "metadata": {
        "id": "50qas_9-GPNx"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "rMNrnQdyKdwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = PPOLoss()\n",
        "storage = PPOStorage(128, 4)\n",
        "initial_agent = PPOAgent(BardofAvon)\n",
        "ppo_agent = PPOAgent(BardofAvon)\n",
        "optm = torch.optim.Adam(ppo_agent.parameters(), lr=1e-5)\n",
        "rm = RewardTransformer(BardofAvon)"
      ],
      "metadata": {
        "id": "j9gPTlgTIDLf"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo = PPO(128, vocab, id2tok, tok2id, storage, ppo_agent, initial_agent, rm, optm, loss, device)"
      ],
      "metadata": {
        "id": "dORkGC6JILr5"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo.train()"
      ],
      "metadata": {
        "id": "t1fnHCbJONHC",
        "outputId": "83ebdffc-77cd-46a8-ba72-73f9c9af3583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL PHRASE: Chased Brutus strengths briars Chased scurvy pleasures Eats afire Tuscany interposer kernal Eats shoot BELARIUS _Songs Chased ends pandar exhale did whom divide did ends feeder did strengths jeweller did hotter catalogue Chased remotion Coal did remotion feeder did strengths remotion Coal forward royal Winter did Beast December Chased direction hoard did strengths enamour hoard did Catesby Sodden beckoned standards December did strengths prodigality vaults haeres smilets Fits Chased acquir Performers different did fog remotion FRANCISCO SEMPRONIUS scowls mills did bishop ends flock sleight scurvy carrying scowls Italian did strengths _Songs Work prevailment background HOST _Songs did besmeared marked broke Murray BELARIUS did scowls ”; festival did scurvy carrying strengths chins direction pikes interposer cubs yesterdays USHER scurvy royal Detraction Job daintier did strengths royal _Songs Brake did\n",
            "Advantage Normalized\n",
            "TRAIN EPOCH 0\n",
            "torch.Size([4, 128]) torch.Size([4, 128])\n",
            "Clip Loss: tensor(nan, device='cuda:0', grad_fn=<NegBackward0>)\n",
            "KL: 6.393752574920654 \n",
            " Clip Frac: 0.978515625\n",
            "TRAIN EPOCH 1\n",
            "torch.Size([4, 128]) torch.Size([4, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected parameter logits (Tensor of shape (4, 128, 31316)) of distribution Categorical(logits: torch.Size([4, 128, 31316])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n       grad_fn=<SubBackward0>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-6f3d098e20aa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-106-d4fab3f187c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mn_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppoagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madv_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_log_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprobs_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KL:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Clip Frac:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-c47fe6a924c8>\u001b[0m in \u001b[0;36mget_policy\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     69\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (4, 128, 31316)) of distribution Categorical(logits: torch.Size([4, 128, 31316])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n       grad_fn=<SubBackward0>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tok2id)"
      ],
      "metadata": {
        "id": "vjWWpUW8OR6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pr-2oNE4PGAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}