{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1 - Transformers\n",
        "## Deep Learning Curriculum - Jacob Hilton"
      ],
      "metadata": {
        "id": "K8iD7_b76zdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theory\n",
        "\n",
        "* Here are some first principle questions to answer:\n",
        "1. What is different architecturally from the Transformer, vs a normal RNN, like an LSTM? (Specifically, how are recurrence and time managed?)\n",
        "\n",
        "    In an RNN, input data is processed sequentially. Given an initial hidden-state and the first input token, the RNN rolls-out, where each subsequent copy of the neural network receives the hidden-state of the previous NN (supposedly containing the memory of the network and the context prior to the current position) and the current token being processed. Thus, the RNN deals with the sequential nature of the input data by literally processing the input sequentially. Other than the fact that RNNs keep a memory of the previous outputs used in present computations, in the form of hidden states, there architecture is very similar to an MLP, in that they have input, hidden and output layers. There is no need to positionally encode the tokens since they are being processed one at a time in the position in which they find themselves.\n",
        "\n",
        "    In a Transformer network, input sequences are not processed sequentially. Given an input sequence of tokens, the attention mechanism operates over the whole sequence at once. Ouery, key and value representations of the encoded tokens are generated using the learned linear projection matrices. The attention pattern is calculated from the dot-product between queries and keys. After softmax is applied to the attention pattern column-wise, each element in each column is used as the scalers on a linear combination of the values in order to update current embeddings. After updating current embeddings, these are passed through an MLP. As the whole sequence is processed at once, in parallel, in order to capture the structure of the data, these must be positionally encoded.\n",
        "\n",
        "2. Attention is defined as, $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})V$. What are the dimensions for Q, K, and V? Why do we use this setup? What other combinations could we do with (Q,K) that also output weights?\n",
        "\n",
        "    $dim(Q) = n_{tokens} \\times d_k$\n",
        "\n",
        "    $dim(K) = n_{tokens} \\times d_k$\n",
        "\n",
        "    $dim(V) = n_{tokens} \\times d_v$\n",
        "\n",
        "    Specifically in the paper $d_k = d_v = 64$. In terms of possible combinations of $Q$ and $K$, the only necessary condition is that there is one key and query vector for each token in the sequence and that these vectors are of the same size. Since at the end of a multi-head attention block the outputs from each head are concatenated, it is useful to have $d_k = d_{model}/h$ where $h$ is the number of heads. Thus, as stated in the paper, you can use values for $d_k$ such as 16, 32, 128, 64, 512 (for 32, 16, 4, 8 and 1 head, respectively). Also, due to this, you cannot use $d_k \\gt d_{model}$, additionally beacuse it would make no sense to project the embeddings to a space higher than their own.\n",
        "\n",
        "3. Are the dense layers different at each multi-head attention block? Why or why not?\n",
        "\n",
        "    All the dense layers between the multi-head attention blocks are the same, in the sense that they have the same architecture, though their learned parameter values are of course different. They all act in the same vector spaces, and are used to capture more complex non-linear relationships in the data that the multi-head attention would have difficulty capturing. As they are applied position-wise, the input layer is of size $d_{model}/h$ and output layers must be of $d_{model}$ size. For simplicities sake the hidden-layer size is kept the same.\n",
        "\n",
        "4. Why do we have so many skip connections, especially connecting the input of an attention function to the output? Intuitively, what if we didn't?\n",
        "\n",
        "    We have these skip connections firstly because they represent the updating of the initial embeddings by the the linearly combined value vectors and secondly in order to reduce the problem of vanishing gradients. It is to eliminate loss of information in the residual stream."
      ],
      "metadata": {
        "id": "wwX8Jxoi6-4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code\n",
        "\n",
        "* Now we'll actually implement the code. Make sure each of these is completely correct - it's very easy to get the small details wrong.\n",
        "  * Implement the positional embedding function first.\n",
        "  * Then implement the function which calculates attention, given (Q,K,V) as arguments.\n",
        "  * Now implement the masking function.\n",
        "  * Put it all together to form an entire attention block.\n",
        "  * Finish the whole architecture.\n",
        "* If you get stuck, The Annotated Transformer may help, but don't just copy-paste the code.\n",
        "* To check you have the attention mask set up correctly, train your model on a toy task, such as reversing a random sequence of tokens. The model should be able to predict the second half of the sequence, but not the first.\n",
        "* Finally, train your model on the complete works of William Shakespeare.\n",
        "  * Tokenize the corpus by splitting at word boundaries (re.split(r\"\\b\", ...)).\n",
        "  * Make sure you don't use overlapping sequences as this can lead to overfitting."
      ],
      "metadata": {
        "id": "76X6pjHBHoK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Packages"
      ],
      "metadata": {
        "id": "433WPcBtINx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IREMYSLJ6tfg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "  def forward(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "HX2fvehXdS5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "  def forward(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "wbgbnX7Fdn3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, batch, vocab_size, d_model=512, n_heads=8, hidden_size=2048):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    # initial hyperparams\n",
        "    self.d_model = d_model\n",
        "    self.batch = batch\n",
        "    self.n_heads = n_heads\n",
        "    self.d_k = d_model/n_heads\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, self.d_model)\n",
        "\n",
        "    # multi-head attention blocks\n",
        "    self.Wq, self.Wk, self.Wv, self.W0 = self.multiHeadAttention()\n",
        "\n",
        "    # layernorm\n",
        "    self.layernorm1 = nn.LayerNorm(self.d_model)\n",
        "\n",
        "    # position-wise feed forward\n",
        "    self.ff = self.mlp()\n",
        "\n",
        "\n",
        "\n",
        "  def __op_pos_enc(self, dim, pos):\n",
        "\n",
        "    return pos/(1e4**((2*dim)/self.d_model))\n",
        "\n",
        "  def _pos_encoding_vec(self, pos):\n",
        "\n",
        "    # for a given position, returns the vector of encodings\n",
        "    pos_vec = np.zeros(self.d_model)\n",
        "    for i in range(int(self.d_model/2)):\n",
        "      pos_vec[2*i] = np.sin(self.__op_pos_enc(i, pos))\n",
        "      pos_vec[2*i + 1] = np.cos(self.__op_pos_enc(i, pos))\n",
        "\n",
        "    return pos_vec\n",
        "\n",
        "  def getPositionalEncoding(self):\n",
        "\n",
        "    pos_encodings = np.zeros((self.batch, self.d_model))\n",
        "\n",
        "    # gets the encoding vector for each position\n",
        "    for pos in range(pos_encodings.shape[0]):\n",
        "      pos_encodings[pos] = self._pos_encoding_vec(pos)\n",
        "\n",
        "    return torch.from_numpy(pos_encodings)\n",
        "\n",
        "  def attention(self, Q, K, V, decoder=True):\n",
        "\n",
        "    den = np.sqrt(self.d_k)\n",
        "\n",
        "    # attention function\n",
        "    dot = torch.matmul(Q, K.t()) / den\n",
        "\n",
        "    # masking if it's a decoder network\n",
        "    if decoder:\n",
        "      dot = self._masking(dot)\n",
        "    att = F.softmax(dot, dim = 1)\n",
        "    att = torch.matmul(att, V)\n",
        "\n",
        "    return att\n",
        "\n",
        "  def _masking(self, att_patt):\n",
        "\n",
        "    rows, cols = att_patt.shape\n",
        "    temp = torch.ones(rows, cols)\n",
        "    temp = torch.tril(temp)\n",
        "    mask = temp == 0\n",
        "\n",
        "    # masking upper triangle with -inf\n",
        "    mask_mat = torch.masked_fill(att_patt, mask, -np.inf)\n",
        "\n",
        "    return mask_mat\n",
        "\n",
        "  def mlp(self):\n",
        "\n",
        "    # dense layer\n",
        "    mlp = nn.Sequential(\n",
        "        nn.Linear(self.d_model, self.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.hidden_size, self.d_model)\n",
        "    )\n",
        "\n",
        "    return mlp\n",
        "\n",
        "  def multiHeadAttention(self):\n",
        "\n",
        "    # queries\n",
        "    Wq = nn.ModuleList([nn.Linear(self.d_model, self.d_k) for _ in range(self.n_heads)])\n",
        "\n",
        "    # keys\n",
        "    Wk = nn.ModuleList([nn.Linear(self.d_model, self.d_k) for _ in range(self.n_heads)])\n",
        "\n",
        "    # values\n",
        "    Wv = nn.ModuleList([nn.Linear(self.d_model, self.d_k) for _ in range(self.n_heads)])\n",
        "\n",
        "    # last linear layer\n",
        "    W0 = nn.ModuleList([nn.Linear(self.d_model, self.s_model) for _ in range(self.n_heads)])\n",
        "\n",
        "    return Wq, Wk, Wv, W0\n",
        "\n",
        "  def forward(self):\n",
        "    pass\n",
        ""
      ],
      "metadata": {
        "id": "OmcLCbI7IeHG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "EjkCywPIWbTq",
        "outputId": "aa5d98c1-31dc-4541-b27f-5312c1861028"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Transformer.__init__() missing 1 required positional argument: 'vocab_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f683be37fbcb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() missing 1 required positional argument: 'vocab_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = model.getPositionalEncoding()"
      ],
      "metadata": {
        "id": "RdbIfLp6WeeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "U2oHrojqWiD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cax = plt.matshow(a)\n",
        "plt.gcf().colorbar(cax)"
      ],
      "metadata": {
        "id": "IEqmuCqacUkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = torch.randn((5, 64))\n",
        "K = torch.randn((5, 64))\n",
        "V = torch.randn((5, 64))"
      ],
      "metadata": {
        "id": "lKT-4Y_rcWbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = model.attention(Q, K, V)"
      ],
      "metadata": {
        "id": "VXqBRxvHjmAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SmallNet, self).__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(5)])\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    out_list = []\n",
        "    for i, l in enumerate(self.layers):\n",
        "      out = l(X)\n",
        "      out_list.append(out)\n",
        "    return out_list"
      ],
      "metadata": {
        "id": "5TrBp28pmceN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_net = SmallNet()"
      ],
      "metadata": {
        "id": "stlBCTkyUJ9w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones(10)"
      ],
      "metadata": {
        "id": "I2HpEXlMUn-C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez4zEAD3UgWl",
        "outputId": "1374d252-f8be-4597-8e88-380340ae0475"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([ 0.3752, -0.0617,  0.0061, -0.3177,  0.3214, -0.8134,  0.1087, -0.3988,\n",
              "         -0.5155, -0.0663], grad_fn=<ViewBackward0>),\n",
              " tensor([-0.7972,  0.9918, -0.5188,  0.4179,  1.1942,  0.0958,  0.6256, -0.3470,\n",
              "         -0.1079,  0.6537], grad_fn=<ViewBackward0>),\n",
              " tensor([-0.6397,  0.6054, -0.0701, -0.2502, -0.3925,  0.4335, -0.5914,  0.4660,\n",
              "         -0.2802,  0.4107], grad_fn=<ViewBackward0>),\n",
              " tensor([ 1.0603,  0.2111, -0.2385,  0.2580,  0.0450, -0.4529, -0.6450,  0.2023,\n",
              "         -0.0367, -1.1947], grad_fn=<ViewBackward0>),\n",
              " tensor([ 0.0514,  0.8784, -0.3908, -1.0010, -0.4124, -0.4319, -0.8391,  0.5553,\n",
              "         -0.2332,  0.5662], grad_fn=<ViewBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mx-hFzTCUlGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}