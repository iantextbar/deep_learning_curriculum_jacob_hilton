{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4 - Optimization\n",
        "## Deep Learning Curriculum - Jacob Hilton"
      ],
      "metadata": {
        "id": "qoBOLzZMALZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "c9enesK2AKyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Set up a testbed using the setup from the NQM paper, where the covariance matrix of the gradient and the Hessian are both diagonal. You can use the same defaults for these matrices as in the paper, i.e., diagonal entries of 1, 1/2, 1/3, ... for both (in the paper they go up to 10^4, you can reduce this to 10^3 if experiments are taking too long to run). Implement both SGD with momentum and Adam."
      ],
      "metadata": {
        "id": "HLkkrdhOAVuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Notes:\n",
        "\n",
        "Consider that the parameters $\\theta_i$ are initialized such that $\\theta_i \\sim N(0, 1)$. Consider that $E(\\theta_i^{1}) = E(\\theta_i^0 - \\alpha g(\\theta_i)) = E(\\theta_i^0) - \\alpha E(h_i \\theta_i^0 + \\epsilon_i) = (1 - \\alpha h_i)E(\\theta_i^0) = 0$ thus gradient descent will not alter the mean of the parameter distribution. Thus, the only thing that will be ordered over the course of the updates will be the covariance matrix, due to the introduction of noise in the parameters stemming from the stochasticity of SGD. Thus in order to follow the evolution of the parameters we only need to look at the evolution of the variance of the parameters.\n",
        "\n",
        "$$Var(\\theta_i^t) = v_i^t \\\\ v_i^t = (1 - \\alpha h_i)^2 v_{i}^{t-1} + \\alpha^2 \\sigma_i^2$$\n",
        "\n",
        "where $\\sigma_i$ is the noise variance. Through recursion:\n",
        "\n",
        "$$0: v_i^0 \\\\\n",
        "1: v_i^1 = (1 - \\alpha h_i)^2 v_0 + \\alpha^2 \\sigma_i^2 \\\\\n",
        "2: v_i^2 = (1 - \\alpha h_i)^4 v_0 + (1 - \\alpha h_i)^2 \\alpha^2 \\sigma_i^2 + \\alpha^2 \\sigma_i^2 \\\\\n",
        "\\vdots \\\\\n",
        "t: v_i^t = (1 - \\alpha h_i)^{2t} v_0 + \\sum_{k=0}^{t-1} \\alpha^2 \\sigma_i^2 (1 - \\alpha h_i)^{2 k }$$\n",
        "\n",
        "considering the sum of the geometric series $\\sum_{k=0}^{t-1} \\alpha^2 \\sigma_i^2 (1 - \\alpha h_i)^{2 k } = \\frac{\\alpha^2 \\sigma_i^2(1 - (1 - \\alpha h_i)^{2t})}{1 - (1 - \\alpha h_i)^2}$ thus our final equation for t-step-ahead parameters is:\n",
        "\n",
        "$$v_i^t = (1 - \\alpha h_i)^{2t} (v_0 - \\frac{\\alpha^2 \\sigma_i^2}{1 - (1 - \\alpha h_i)^2}) + \\frac{\\alpha^2 \\sigma_i^2}{1 - (1 - \\alpha h_i)^2} \\\\ v_i^t = \\beta^t (v_0 - v^*) + v^*$$ where $$\\beta = (1 - \\alpha h_i)^2 \\\\\n",
        "v^* =  \\frac{\\alpha^2 \\sigma_i^2}{1 - (1 - \\alpha h_i)^2}$$\n",
        "\n",
        "Given the above formula, we want to know how many steps it will take to reach a desired loss, since we want to compare the impact of batch size on this convergence for various optimization algorithms."
      ],
      "metadata": {
        "id": "VqHnuhi2GfiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gradients(size=1e3):\n",
        "\n",
        "  h_vec = 1 / np.arange(1, size+1, 1)\n",
        "  h = np.array(h_vec) * np.eye(len(h_vec))\n",
        "\n",
        "  return torch.from_numpy(h)"
      ],
      "metadata": {
        "id": "vQE81GxeHIvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Xld3fLACqK"
      },
      "outputs": [],
      "source": [
        "loss = lambda h, v: (1/2) * torch.sum(h * v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def t_step_ahead_var(alpha, sigma_sq, hi, t, v0):\n",
        "\n",
        "  beta = (1 - alpha*hi)**2\n",
        "  v_star = ((alpha**2)*sigma_sq) / (1 - beta)\n",
        "\n",
        "  return (beta**t) * (v0 - v_star) + v_star"
      ],
      "metadata": {
        "id": "j_Wed7JbDaZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def steps_to_loss(alpha, sigma_sq, hi, v0, loss):\n",
        "\n",
        "  high = 1e20\n",
        "  low = 0\n",
        "\n",
        "  while low < high:\n",
        "\n",
        "    mid = round((low + high) / 2)\n",
        "    v_mid = t_step_ahead_var(alpha, sigma_sq, hi, mid, v0)\n",
        "    l = loss(hi, v_mid)\n",
        "\n",
        "    # if we have reduced loss more than we needed\n",
        "    if l<loss:\n",
        "      high=mid\n",
        "    # if we need more steps\n",
        "    else:\n",
        "      low=mid\n",
        "\n",
        "  return low"
      ],
      "metadata": {
        "id": "5UCgarO6DaDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_curve(alpha, sigma_sq, hi, max_t, v0):\n",
        "  losses = []\n",
        "  for t in range(1, max_t+1):\n",
        "    vt = t_step_ahead_var(alpha, sigma_sq, hi, t, v0)\n",
        "    l = loss(hi, vt)\n",
        "    losses.append(l)\n",
        "  return losses"
      ],
      "metadata": {
        "id": "poIopML3IOkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_alphas(sigma_sq, hi, v0, loss, low_a = 1e-10, high_a = 0.75):\n",
        "\n",
        "  alphas = np.arange(low_a, high_a, 1000)\n",
        "  best_steps = 1e50\n",
        "  best_alpha = None\n",
        "  for a in alphas:\n",
        "    steps = steps_to_loss(a, sigma_sq, hi, v0, loss)\n",
        "    if steps < best_steps:\n",
        "      best_steps = steps\n",
        "      best_alpha = a\n",
        "\n",
        "  return best_alpha, best_steps"
      ],
      "metadata": {
        "id": "nVLGiSZ5-SjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Momentum"
      ],
      "metadata": {
        "id": "mQxo3Jx9I9dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following Appendix B, and the implementation: https://github.com/gd-zhang/noisy-quadratic-model/blob/master/nqm.ipynb"
      ],
      "metadata": {
        "id": "UnY2JTkIJti0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "yr9f8X7TWqiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_momentum_dynamics(h, sigma_sq, s_init, lrate, k, momentum=0.0):\n",
        "    ndim = h.shape[0]\n",
        "\n",
        "    init_p = torch.zeros((ndim, 3))\n",
        "    init_p[:, 0] = s_init\n",
        "\n",
        "    x = lrate*h\n",
        "    noise = ((lrate)**2 * sigma_sq).unsqueeze(-1).expand(ndim, 3)\n",
        "\n",
        "    # defining the transition matrix (see eqn.(21) in the paper)\n",
        "    trans_mat = torch.zeros((ndim, 3, 3))\n",
        "    trans_mat[:, :, 1] = momentum ** 2\n",
        "    trans_mat[:, 0, 0] = (1 - x) ** 2\n",
        "    trans_mat[:, 1, 0] = x ** 2\n",
        "    trans_mat[:, 2, 0] = - (1 - x) * x\n",
        "    trans_mat[:, 0, 2] = 2 * (1 - x) * momentum\n",
        "    trans_mat[:, 1, 2] = -2 * x * momentum\n",
        "    trans_mat[:, 2, 2] = (1 - 2 * x) * momentum\n",
        "\n",
        "    # eigen-decomposition\n",
        "    e, v = torch.linalg.eig(trans_mat)\n",
        "    v_inv = torch.linalg.inv(v)\n",
        "\n",
        "    trans_init_p = torch.matmul(v_inv, init_p.unsqueeze(-1).type(torch.complex64)).squeeze()\n",
        "    trans_noise = torch.matmul(v_inv, noise.unsqueeze(-1).type(torch.complex64)).squeeze()\n",
        "\n",
        "    p_star = trans_noise / (1 - e)\n",
        "\n",
        "    trans_final_p = e ** k * (trans_init_p - p_star) + p_star\n",
        "    final_p = torch.matmul(v, trans_final_p.unsqueeze(-1)).squeeze()\n",
        "    return final_p[:, 0].real"
      ],
      "metadata": {
        "id": "9Mx5ZQ0kVOqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_to_loss_wm(h, sigma_sq, s_init, lrate, momentum, C, counts=None, prec=None):\n",
        "\n",
        "  high = 10000000\n",
        "  low = 0\n",
        "\n",
        "  while low < high - 1:\n",
        "    mid = int(math.floor((low + high) / 2))\n",
        "\n",
        "    s = simulate_momentum_dynamics(h, sigma_sq, s_init, lrate, mid, momentum=momentum, prec=prec)\n",
        "    L = loss(h, s, counts)\n",
        "    if L < C:\n",
        "      high = mid\n",
        "    else:\n",
        "      low = mid\n",
        "\n",
        "  return low"
      ],
      "metadata": {
        "id": "KSlOxAJCWoBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_alphas_and_momentum(h, sigma_sq, s_init, C, low_a = 1e-10, high_a = 0.75, max_momentum = 0.95):\n",
        "  alphas = np.arange(low_a, high_a, 100)\n",
        "  momentums = np.arange(0, max_momentum, 20)\n",
        "\n",
        "  best_steps = 1e50\n",
        "  best_alpha = None\n",
        "  best_moment = None\n",
        "\n",
        "  for a in alphas:\n",
        "    for m in momentums:\n",
        "\n",
        "      steps = time_to_loss_wm(h, sigma_sq, s_init, a, m, C)\n",
        "\n",
        "      if steps < best_steps:\n",
        "\n",
        "        best_steps = steps\n",
        "        best_alpha = a\n",
        "        best_moment = m\n",
        "\n",
        "  return best_alpha, best_moment, best_steps"
      ],
      "metadata": {
        "id": "89HGtIKJHOhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam\n",
        "\n",
        "As in section 3.3, if we use $P = H^p$ then the preconditioned case works exactly like the non preconditioned case, but with hessian $\\tilde{H} = P^{-\\frac{1}{2}} H P^{-\\frac{1}{2}}$ and gradient covariance $\\tilde{C} = P^{-\\frac{1}{2}} C P^{-\\frac{1}{2}}$."
      ],
      "metadata": {
        "id": "RfvMTIwvXDLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_hessian(h, p=0.5):\n",
        "\n",
        "  P = h**p\n",
        "  P_exp = P ** (-.5)\n",
        "  new_h = torch.matmul(torch.matmul(P_exp, h), P_exp)\n",
        "\n",
        "  return new_h"
      ],
      "metadata": {
        "id": "9NBmixfAXsvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create a method for optimizing learning rate schedules. You can either use dynamic programming using equation (3) as in the paper (see footnote on page 7), or a simpler empirical method such as black-box optimization (perhaps with simpler schedule).\n",
        "\n",
        "Notes: We want to find the optimal piecewise constant learning rate schedule, i.e. $lr_1$ for the first 20 epochs, then $lr_2$ for the next 30 etc. In the paper we see that this is done first by creating an optimization algorithm that minimizes loss by finding the best learning rate schedule given the number of steps. Then binary search is conducted on the number of steps to find the schedule that reaches the target loss in the least amount of steps.\n",
        "\n",
        "The algorithm used to find the optimal learning rate schedule is the BFGS algorithm, which is a second-order optimization algorithm. It does not calculate the hessian directly, instead calculating approximations via the gradient."
      ],
      "metadata": {
        "id": "-56zLnATC37m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import Bounds, minimize\n",
        "from jax import grad\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "UYtycL9mviwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_jax = lambda h, v: (1/2) * jnp.sum(h * v)"
      ],
      "metadata": {
        "id": "aH-UZjZcjyDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gradients_jax(size=1e3):\n",
        "\n",
        "  h_vec = 1 / np.arange(1, size+1, 1)\n",
        "  h = np.array(h_vec) * jnp.eye(len(h_vec))\n",
        "\n",
        "  return h"
      ],
      "metadata": {
        "id": "CeLWMc8VimUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def t_step_ahead_var_jax(alpha, sigma_sq, hi, t, v0):\n",
        "\n",
        "  beta = (1 - alpha*hi)**2\n",
        "  v_star = ((alpha**2)*sigma_sq) / (1 - beta)\n",
        "\n",
        "  return (beta**t) * (v0 - v_star) + v_star"
      ],
      "metadata": {
        "id": "jZwho3HHi_CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def t_ahead_var_with_lrs(alphas, sigma_sq, hi, t_intervals, v0):\n",
        "  v = v0\n",
        "  alphas = jnp.exp(alphas)\n",
        "  combs = zip(alphas, t_intervals)\n",
        "  for a, t in combs:\n",
        "    v = t_step_ahead_var_jax(a, sigma_sq, hi, t, v)\n",
        "  return loss_jax(hi, v)"
      ],
      "metadata": {
        "id": "8wcnr8tFC5A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_alphas(sigma_sq, hi, t_intervals, v0, max_bound):\n",
        "\n",
        "  init_alphas = -1 * jnp.ones(len(t_intervals))\n",
        "  loss_wrap = lambda alphas: t_ahead_var_with_lrs(alphas, sigma_sq, hi, t_intervals, v0)\n",
        "  gradient = grad(t_ahead_var_with_lrs)\n",
        "  gradient_wrap = lambda alphas: gradient(alphas, sigma_sq, hi, t_intervals, v0)\n",
        "  bounds = Bounds(-np.inf, max_bound)\n",
        "  result = minimize(loss_wrap, init_alphas, jac=gradient_wrap, bounds=bounds)\n",
        "\n",
        "  return result.x"
      ],
      "metadata": {
        "id": "o1n5rvgEtBxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def steps_to_loss_pwc(sigma_sq, hi, v0, loss, max_bound, pieces=50):\n",
        "\n",
        "  high = 1e20\n",
        "  low = 0\n",
        "  best_alphas = None\n",
        "  min_steps = None\n",
        "\n",
        "  while low < high:\n",
        "\n",
        "    mid = round((low + high) / 2)\n",
        "    mid = int(min(mid, 2**31 - 1))\n",
        "    t_intervals = mid * jnp.ones(pieces)\n",
        "    alphas = optimal_alphas(sigma_sq, hi, t_intervals, v0, max_bound)\n",
        "    l = t_ahead_var_with_lrs(alphas, sigma_sq, hi, t_intervals, v0)\n",
        "\n",
        "    # if we have reduced loss more than we needed\n",
        "    if l<loss:\n",
        "      high=mid\n",
        "      best_alphas = alphas\n",
        "      min_steps = mid\n",
        "\n",
        "    # if we need more steps\n",
        "    else:\n",
        "      low=mid\n",
        "\n",
        "  return min_steps, best_alphas"
      ],
      "metadata": {
        "id": "0GziKvc75e0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hi = make_gradients_jax()"
      ],
      "metadata": {
        "id": "cdSRQfOLddcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = 2 ** jnp.arange(4, 12)\n",
        "s_init = jnp.ones(10**3)\n",
        "target_loss = 0.01\n",
        "\n",
        "for i, bs in enumerate(batch_sizes):\n",
        "  sigma_sq = hi / bs\n",
        "  _, lrates = steps_to_loss_pwc(sigma_sq, hi, s_init, target_loss, 2)\n",
        "  #if bs.item() != 64: # TODO: figure out what's wrong with bs=64\n",
        "  plt.semilogy(lrates.detach().numpy(), label=\"BS \" + str(bs), basey=2)\n",
        "plt.xlabel('Pieces')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.ylim(ymax=4)\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=3);"
      ],
      "metadata": {
        "id": "nOvu_L1G7T9S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check that at very small batch sizes, the optimal learning rate scales with batch size as expected: proportional to the batch size for SGD, proportional to the square root of the batch size for Adam."
      ],
      "metadata": {
        "id": "gxBODPkSC8OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [2 ** i for i in range(2, 22)]\n",
        "hi = make_gradients()\n",
        "v0 = torch.ones(10**3)\n",
        "target_loss = 0.1\n",
        "\n",
        "num_steps = []\n",
        "optimal_lrs = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    sigma_sq = hi / bs\n",
        "    optimal_alpha, steps = grid_search_alphas(sigma_sq, hi, v0, target_loss)\n",
        "    num_steps.append(steps)\n",
        "    optimal_lrs.append(optimal_alpha)"
      ],
      "metadata": {
        "id": "f4URa-3cC-Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.loglog(batch_sizes, optimal_lrs, basex=2, basey=2)\n",
        "#plt.ylim(2 ** (-14), 2 ** 3)\n",
        "plt.xlabel(\"Batch size\")\n",
        "plt.ylabel(\"Optimal Learning Rate\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "J1MvOfiirg4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [2 ** i for i in range(2, 22)]\n",
        "hi = make_gradients()\n",
        "v0 = torch.ones(10**3)\n",
        "target_loss = 0.1\n",
        "\n",
        "num_steps_mom = []\n",
        "optimal_lrs_mom = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    sigma_sq = hi / bs\n",
        "    optimal_alpha, optimal_moment, steps_to_target = grid_search_alphas_and_momentum(hi, sigma_sq, v0, target_loss)\n",
        "    num_steps_mom.append(steps_to_target)\n",
        "    optimal_lrs_mom.append(optimal_alpha)"
      ],
      "metadata": {
        "id": "ss6YU8uVrpE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.loglog(batch_sizes, optimal_lrs_mom, basex=2, basey=2)\n",
        "plt.ylim(2 ** (-14), 2 ** 3)\n",
        "plt.xlabel(\"Batch size\")\n",
        "plt.ylabel(\"Optimal Learning Rate\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "S7cRVQHZsYaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Look at the relationship between the batch size and the number of steps to reach a target loss. Study the effects of momentum and using Adam on this relationship."
      ],
      "metadata": {
        "id": "gY0hXEGNDB9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.loglog(batch_sizes, num_steps, basex=2, basey=2)\n",
        "plt.xlabel(\"Batch size\")\n",
        "plt.ylabel(\"Steps until target loss\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "uLu6yyHhDCxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.loglog(batch_sizes, num_steps_mom, basex=2, basey=2)\n",
        "plt.xlabel(\"Batch size\")\n",
        "plt.ylabel(\"Steps until target loss\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Aiv0D0KMsrw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}